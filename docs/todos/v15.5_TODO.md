Here is the technical checklist for **Phase 7: Pre-Game Extraction Pipeline (Backend & Logic)**.

This roadmap focuses on building the engine that powers the new "Wizard" before we touch the UI code.

---

### **Step 1: LLM Connector Upgrade (Greedy Sampling)**
**Goal:** Allow specific API calls to be "deterministic" (Temperature 0) for data extraction, while keeping narrative calls creative.

- [x] **1.1. Update `LLMConnector` Interface**
    *   **File:** `app/llm/llm_connector.py`
    *   **Action:** Add optional `temperature` and `top_p` arguments to `get_structured_response`.
    *   **Code:**
        ```python
        @abstractmethod
        def get_structured_response(
            self,
            system_prompt: str,
            chat_history: List[Message],
            output_schema: Type[BaseModel],
            temperature: float = 0.7,  # Default to creative
            top_p: float = 0.9
        ) -> BaseModel:
            pass
        ```

- [x] **1.2. Update `OpenAIConnector` & `GeminiConnector`**
    *   **Files:** `app/llm/openai_connector.py`, `app/llm/gemini_connector.py`
    *   **Action:** Pass these new arguments into the respective SDK client calls (`client.chat.completions.create` or `model.generate_content`).

---

### **Step 2: Extraction Schemas (The Target Data)**
**Goal:** Define strict Pydantic models that the LLM must fill out based on the user's text input.

- [x] **2.1. Create `app/setup/schemas.py`**
    *   **Action:** Define the containers for the extraction results.
    *   **Snippet:**
        ```python
        from typing import List, Dict, Any, Optional
        from pydantic import BaseModel, Field
        from app.tools.schemas import LocationCreate, MemoryUpsert, NpcSpawn

        class CharacterExtraction(BaseModel):
            name: str = Field(..., description="Name of the protagonist.")
            visual_description: str = Field(..., description="Physical appearance.")
            bio: str = Field(..., description="Short backstory/biography.")
            
            # We ask for a flat dict of stats; Python will map them to the Template later
            suggested_stats: Dict[str, Any] = Field(..., description="Key-value pairs of attributes/skills inferred from text (e.g. {'Strength': 18}).")
            inventory: List[str] = Field(default_factory=list, description="List of starting items.")
            
            # Advanced: Sidekicks/Pets defined in the bio
            companions: List[NpcSpawn] = Field(default_factory=list, description="Familiars, pets, or followers mentioned.")

        class WorldExtraction(BaseModel):
            starting_location: LocationCreate
            lore: List[MemoryUpsert] = Field(..., description="Key facts about the world mentioned in the text.")
            initial_npcs: List[NpcSpawn] = Field(default_factory=list, description="NPCs present in the starting scene.")
        ```

---

### **Step 3: The WorldGen Service (The Logic)**
**Goal:** The brain of the Wizard. It orchestrates the LLM calls.

- [x] **3.1. Create `app/setup/world_gen_service.py`**
    *   **Class:** `WorldGenService`
    *   **Dependency:** Needs `LLMConnector`.

- [x] **3.2. Implement `extract_world_data`**
    *   **Input:** `world_description: str`
    *   **Logic:**
        1.  Construct prompt: *"Analyze this setting description. Extract the starting location (visuals, exits) and key lore facts."*
        2.  Call LLM with **Temperature 0.1**.
        3.  Return `WorldExtraction` object.

- [x] **3.3. Implement `extract_character_data`**
    *   **Input:** `char_description: str`, `stat_template: StatBlockTemplate`
    *   **Logic:**
        1.  Convert `stat_template` to a simplified JSON string (so the LLM knows valid skills/stats).
        2.  Construct prompt: *"Map this character description to the following Ruleset. Infer missing stats based on the archetype. Return specific values."*
        3.  Call LLM with **Temperature 0.1**.
        4.  Return `CharacterExtraction` object.

- [x] **3.4. Implement `generate_opening_crawl`**
    *   **Input:** `CharacterExtraction`, `WorldExtraction`
    *   **Logic:**
        1.  Construct prompt: *"Write the opening scene for a roleplaying game. The player is [Name], standing in [Location]. The atmosphere is [Tone]. End with 'What do you do?'"*
        2.  Call LLM with **Temperature 0.8** (Creative).
        3.  Return `str`.

---

### **Step 4: Prompt Templates**
**Goal:** Instructions for the extraction jobs.

- [x] **4.1. Update `app/prompts/templates.py`**
    *   **Add:** `WORLD_EXTRACTION_PROMPT`
    *   **Add:** `CHARACTER_EXTRACTION_PROMPT`
        *   *Tip:* Include instructions like *"If the user mentions a specific item (e.g. 'Laser Sword'), add it to inventory. If they mention a specific companion (e.g. 'My robot dog'), add to companions."*
    *   **Add:** `OPENING_CRAWL_PROMPT`

---

### **Step 5: Cleanup & Deprecations**
**Goal:** Remove the old "Session Zero" artifacts.

- [x] **5.1. Database Schema Update**
    *   **File:** `app/database/repositories/prompt_repository.py`
    *   **Action:** Remove `initial_message` column from `prompts` table (or just ignore it).
    *   **Note:** Since we generate the opening message dynamically based on the extracted world, the static `initial_message` is obsolete.

- [x] **5.2. Remove Old Tools**
    *   **Files:**
        *   `app/tools/builtin/request_setup_confirmation.py`
        *   `app/tools/builtin/end_setup_and_start_gameplay.py`
    *   **Action:** Delete these files. The Wizard handles the transition, so the LLM never needs to "request confirmation" inside the chat.

- [x] **5.3. Update `SetupManifest`**
    *   **File:** `app/setup/setup_manifest.py`
    *   **Action:** This class might become irrelevant if the Wizard handles everything in memory before commit.
    *   **Decision:** Keep it for now as a utility to validate the data before the Wizard calls "Commit", but strip out the "confirmation_pending" logic.